配置文件setting里添加

```python

# 启用自定义管道（按优先级排序）
ITEM_PIPELINES = {

    'fang.pipelines.CategoryImagesPipeline': 300,
    'fang.pipelines.CategoryFilesPipeline': 301,
}

# 图片存储配置
IMAGES_STORE = './data/images'  # 根目录
IMAGES_THUMBS = {              # 缩略图生成（可选）
    'small': (50, 50),
    'big': (270, 270),
}

# 文件存储配置
FILES_STORE = './data/files'
```

爬虫部分代理，抓取某壁纸网站，图片管道抓图片，文件管道抓MP4

```python
import scrapy
import re
import json
from fang.items import ProductItem

class ImgspiderSpider(scrapy.Spider):
    name = "imgspider"
    allowed_domains = []
    start_urls = ["https://2t6y.mydown.com/yuanqidesktop/tj.html?softid=585&tid1=18&tid2=1001&tod1=8319&bd_vid=11288380602015107237"]

    def parse(self, response):
        item = ProductItem()
        title_img = response.xpath('//script/text()').extract_first()
        json_data = re.findall('{name:"(\w+)",image:"(.*?)"}', title_img)
        for title,image_urls in json_data:
            item['title'] = title
            # encode('latin-1')将字符串无损转换为字节（确保处理转义字符）
            #decode('unicode_escape') 将 \u002F 等Unicode转义符还原为 /
            img_list = image_urls.split('",video:')
            if len(img_list) > 1:
                item['file_urls'] = img_list[-1].strip('"').encode('latin-1').decode('unicode_escape')
                print(item['file_urls'])
            else:
                item['file_urls'] = ''
            item['image_urls'] = img_list[0].encode('latin-1').decode('unicode_escape')
            item['category'] = 'my_type'
            # print(item['image_urls'])
            yield item



        # for name_index in range(len(title_img)-1):
        #     item['image_urls'] = title_img[name_index].split(',video:')[0]
        #     item['title'] = title_img[name_index+1]
        #     item['category'] = 'my_type'
        #     print(item['image_urls'].split('/')[-1])
        #     yield item
```

pipelines这个部分

```python
from scrapy.pipelines.images import ImagesPipeline
from scrapy.http import Request

class CategoryImagesPipeline(ImagesPipeline):

    def get_media_requests(self, item, info):
        # 从item中提取图片URL并生成请求
        # for url in item.get('image_urls', []):
        url = item.get('image_urls', '')
        if url:
            yield Request(url, meta={'category': item['category']})

    def file_path(self, request, response=None, info=None, *, item=None):
        # 动态生成分类目录路径
        category = request.meta['category']
        image_guid = request.url.split('/')[-1]  # 提取文件名
        return f'{category}/{image_guid}'


from scrapy.pipelines.files import FilesPipeline

class CategoryFilesPipeline(FilesPipeline):
    def get_media_requests(self, item, info):
        # for url in item.get('file_urls', []):
        url = item.get('file_urls', '')
        if url:
            yield Request(url, meta={'category': item['category']})

    def file_path(self, request, response=None, info=None, *, item=None):
        category = request.meta['category']
        file_name = request.url.split('/')[-1]
        return f'files/{category}/{file_name}'  # 文件存储路径
```

item这部分

```python
class ProductItem(scrapy.Item):
    title = scrapy.Field()
    category = scrapy.Field()  # 分类字段
    image_urls = scrapy.Field()  # 图片链接列表
    file_urls = scrapy.Field()  # 文件链接列表
   
```

最终效果

![image-20250507221100991](C:\Users\23303\AppData\Roaming\Typora\typora-user-images\image-20250507221100991.png)